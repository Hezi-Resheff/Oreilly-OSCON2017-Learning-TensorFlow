{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TensorFlow for text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To introduce working with text in TensorFlow, we focus on the core modelling components, and create a minimal, contrived text data set that will let us get straight to action.\n",
    "\n",
    "Let's get started, presenting our example data and discussing some key properties of text data sets as we go.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Text sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our simulated data consists of two classes of very short \"sentences\", one comprised of odd digits and the other of even digits (with numbers written in English). We generate sentences built of words representing even and odd numbers. \n",
    "\n",
    "Our goal is to learn to classify each sentence as either odd or even in a supervised text classification task. Of course, we do not really need any machine learning for this simple task — we use this contrived example for illustrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "digit_to_word_map = {1:\"One\",2:\"Two\", 3:\"Three\", 4:\"Four\", 5:\"Five\",\n",
    "                     6:\"Six\",7:\"Seven\",8:\"Eight\",9:\"Nine\"}\n",
    "digit_to_word_map[0]=\"PAD_TOKEN\"\n",
    "\n",
    "times_steps = 6\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,times_steps+1))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2),\n",
    "                                     rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2),\n",
    "                                      rand_seq_len)\n",
    "\n",
    "    #Padding\n",
    "    if rand_seq_len<times_steps:\n",
    "        rand_odd_ints = np.append(rand_odd_ints,\n",
    "                                  [0]*(times_steps-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints,\n",
    "                                   [0]*(times_steps-rand_seq_len))\n",
    "\n",
    "    even_sentences+=[\" \".join([digit_to_word_map[r] for\n",
    "                               r in rand_even_ints])]\n",
    "    odd_sentences+=[\" \".join([digit_to_word_map[r] for\n",
    "                              r in rand_odd_ints])] \n",
    "\n",
    "data = even_sentences+odd_sentences\n",
    "#seqlens = [1,2,3]\n",
    "seqlens *=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "We pad sentences shorter than 6 with zeros (or \"PAD\" symbols) to make all sentences equally-sized (artificially), so they can all be put in one tensor (per batch). This pre-processing step is known as \"zero-padding\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Four Two Two Eight PAD_TOKEN PAD_TOKEN',\n",
       " 'Eight Four Four PAD_TOKEN PAD_TOKEN PAD_TOKEN',\n",
       " 'Two Four Six Two Eight PAD_TOKEN',\n",
       " 'Two Two Six Six Six Two',\n",
       " 'Four Two Six Four Eight Eight',\n",
       " 'Two Six Two Six PAD_TOKEN PAD_TOKEN']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_sentences[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Seven Nine Nine Five PAD_TOKEN PAD_TOKEN',\n",
       " 'One Seven One PAD_TOKEN PAD_TOKEN PAD_TOKEN',\n",
       " 'Seven Three Nine Three Five PAD_TOKEN',\n",
       " 'Three One Five Five Nine One',\n",
       " 'Three Seven Seven One Seven Five',\n",
       " 'Three Nine Five Three PAD_TOKEN PAD_TOKEN']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_sentences[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 5, 6, 6, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original sentence lengths\n",
    "seqlens[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In real applications, where data is not simulated, we would get collection of documents (e.g., one-sentence tweets) and then map each word to an integer ID. \n",
    "\n",
    "Let's create this map — a dictionary with words as keys, and indices as values. We also create the inverse map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Map from words to indices\n",
    "word2index_map ={}\n",
    "index=0\n",
    "for sent in data:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "#Inverse map    \n",
    "index2word_map = dict([(index,word) for word,index in\n",
    "                       word2index_map.items()]) \n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "\n",
    "vocabulary_size = len(index2word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eight': 2,\n",
       " 'five': 7,\n",
       " 'four': 0,\n",
       " 'nine': 6,\n",
       " 'one': 8,\n",
       " 'pad_token': 3,\n",
       " 'seven': 5,\n",
       " 'six': 4,\n",
       " 'three': 9,\n",
       " 'two': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'four',\n",
       " 1: 'two',\n",
       " 2: 'eight',\n",
       " 3: 'pad_token',\n",
       " 4: 'six',\n",
       " 5: 'seven',\n",
       " 6: 'nine',\n",
       " 7: 'five',\n",
       " 8: 'one',\n",
       " 9: 'three'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To wrap-up our data generation for a contrived supervised learning task, we create an array of labels in the one-hot format, train and test sets, a function to generate batches of instances.\n",
    "\n",
    "First, we create some labels split our data to train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding\n",
    "\n",
    "\n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, we create a function that generates batches of sentences, and their respective labels and lengths.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_batch(batch_size,data_x,\n",
    "                       data_y,data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].lower().split()]\n",
    "         for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x,y,seqlens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x,y, lens = get_sentence_batch(batch_size = 8,data_x = train_x, data_y = train_y,data_seqlens = train_seqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 0, 2, 2, 2, 3], [0, 4, 4, 2, 3, 3], [1, 4, 4, 3, 3, 3], [5, 7, 8, 5, 6, 3], [6, 9, 6, 9, 3, 3], [5, 8, 9, 3, 3, 3], [8, 8, 5, 9, 5, 3], [2, 2, 2, 3, 3, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['eight', 'four', 'eight', 'eight', 'eight', 'pad_token'],\n",
       " ['four', 'six', 'six', 'eight', 'pad_token', 'pad_token'],\n",
       " ['two', 'six', 'six', 'pad_token', 'pad_token', 'pad_token'],\n",
       " ['seven', 'five', 'one', 'seven', 'nine', 'pad_token'],\n",
       " ['nine', 'three', 'nine', 'three', 'pad_token', 'pad_token'],\n",
       " ['seven', 'one', 'three', 'pad_token', 'pad_token', 'pad_token'],\n",
       " ['one', 'one', 'seven', 'three', 'seven', 'pad_token'],\n",
       " ['eight', 'eight', 'eight', 'pad_token', 'pad_token', 'pad_token']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[index2word_map[w] for w in sentence] for sentence in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 3, 5, 4, 3, 5, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1]),\n",
       " array([0, 1]),\n",
       " array([0, 1]),\n",
       " array([1, 0]),\n",
       " array([1, 0]),\n",
       " array([1, 0]),\n",
       " array([1, 0]),\n",
       " array([0, 1])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To work with this data in TensorFlow, we create placeholders. The \\_inputs placeholder contains batch_size sentences, each a sequence of length times_steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "num_classes = 2; batch_size=128;\n",
    "tf.reset_default_graph()\n",
    "_inputs = tf.placeholder(tf.int32, shape=[batch_size,times_steps])\n",
    "_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "#seqlens for dynamic calculation\n",
    "_seqlens = tf.placeholder(tf.int32, shape=[batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Supervised word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We next convert our integer sequences into word vectors. We will train word vectors in a a supervised framework, tuning the embedded word vectors to solve the downstream classification task. Here we use vectors of size 64, initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 64\n",
    "with tf.name_scope(\"embeddings\"):\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size,\n",
    "                           embedding_dimension],\n",
    "                          -1.0, 1.0),name='embedding')\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is helpful to think of word embeddings as basic hash tables or look-up tables, mapping words to their dense vector values. Hence the name of the function tf.nn.embedding_lookup() function, which efficiently retrieves the vectors for each word in a given sequence of word indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LSTM and using sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "We create an LSTM cell with tf.contrib.rnn.BasicLSTMCell(), and feed it to tf.nn.dynamic_rnn(). We also give dynamic_rnn() the length of each sequence in a batch of examples, using the \\_seqlens placeholder we created above. \n",
    "\n",
    "dynamic_rnn() uses the sequence length to stop all RNN steps beyond the last real sequence element, and returns the final LSTM outputs we need for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 32\n",
    "with tf.variable_scope(\"lstm\"):\n",
    " \n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(hidden_layer_size)\n",
    "    _, states = tf.nn.dynamic_rnn(lstm_cell, embed,\n",
    "                                        sequence_length = _seqlens,\n",
    "                                        dtype=tf.float32)\n",
    "last_state = states[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " In this task, we want only the last LSTM output state that is not zero -- so, for example, if the length of our original sequence is 5 and we zero-pad it to a sequence of length 15, we extract only the 5-th output state vector from LSTM.\n",
    " \n",
    " Finally, we use this output vector in a linear layer, as in softmax regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal([hidden_layer_size,num_classes],mean=0,stddev=.01))\n",
    "b = tf.Variable(tf.truncated_normal([num_classes],mean=0,stddev=.01))\n",
    "\n",
    "final_pred = tf.matmul(last_state,W) + b\n",
    "\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits  = final_pred,labels = _labels)                         \n",
    "cross_entropy = tf.reduce_mean(softmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We create ops to update gradients and compute accuracy,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(_labels,1),\n",
    "                              tf.argmax(final_pred,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction,\n",
    "                                   tf.float32)))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "and are now ready to train. Note that we give feed_dict batches of input sequences, labels, and sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at 0: 18.75000\n",
      "Accuracy at 100: 100.00000\n",
      "Accuracy at 200: 100.00000\n",
      "Accuracy at 300: 100.00000\n",
      "Accuracy at 400: 100.00000\n",
      "Accuracy at 500: 100.00000\n",
      "Accuracy at 600: 100.00000\n",
      "Accuracy at 700: 100.00000\n",
      "Accuracy at 800: 100.00000\n",
      "Accuracy at 900: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch,seqlen_batch = get_sentence_batch(batch_size,\n",
    "                                                           train_x,train_y,\n",
    "                                                           train_seqlens)\n",
    "        sess.run(train_step,feed_dict={_inputs:x_batch, _labels:y_batch,\n",
    "                                       _seqlens:seqlen_batch})\n",
    "    \n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={_inputs:x_batch,\n",
    "                                               _labels:y_batch,\n",
    "                                               _seqlens:seqlen_batch})\n",
    "            print(\"Accuracy at %d: %.5f\" % (step, acc)) \n",
    "      \n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test,seqlen_test = get_sentence_batch(batch_size,\n",
    "                                                        test_x,test_y,\n",
    "                                                        test_seqlens)\n",
    "        batch_pred,batch_acc = sess.run([tf.argmax(final_pred,1),\n",
    "                                         accuracy],\n",
    "                                        feed_dict={_inputs:x_test,\n",
    "                                                   _labels:y_test,\n",
    "                                                   _seqlens:seqlen_test})\n",
    "        print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Quick hands-on\n",
    "* Try making the embedding dimension much lower (go all the way down to 1) and re-running the training. How does this affect the accuracy progression over the 1000 iterations?\n",
    "* What about the hidden layer size?\n",
    "* Try making the data sequences much longer. Has training speed changed? What about accuracy?\n",
    "* Make the neccessary changes, such that we do not use an additional linear layer, and feed LSTM outputs directly to softmax (hint: dimensions...)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
